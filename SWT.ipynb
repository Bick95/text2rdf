{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text2rdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "# Bert\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "#Bleu score\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Data-sub-set (given restricted nr of triples per sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many triples to train and test system on (min: 1, max: 7)\n",
    "MIN_NUM_TRIPLES = 1\n",
    "MAX_NUM_TRIPLES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths where to retrieve data from\n",
    "DS_BASE_PATH = './WebNLG/'\n",
    "\n",
    "TRAIN_PATH = DS_BASE_PATH + 'train/'\n",
    "TEST_PATH = DS_BASE_PATH + 'dev/'\n",
    "\n",
    "TRAIN_DIRS = [ TRAIN_PATH + str(i) + 'triples/' for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1) ]\n",
    "TEST_DIRS  = [ TEST_PATH  + str(i) + 'triples/' for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print selected directories\n",
    "print('Train dirs:', TRAIN_DIRS)\n",
    "print('Test  dirs:', TEST_DIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Settings (do not touch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originaltripleset_index = 0  # Index of 'originaltripleset' attribute in XML entry\n",
    "modifiedtripleset_index = 1  # Index of 'modifiedtripleset' attribute in XML entry\n",
    "first_lexical_index = 2      # Index as of which verbalizations of RDF triples start in entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of train: train[target_nr_triples][element_id]['target_attribute']\n",
    "train = [[] for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1)]\n",
    "\n",
    "# Documents how many entries there are per number of triples\n",
    "train_stats = [0 for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1)]\n",
    "\n",
    "# Iterate through all files per number of triples and per category and load data\n",
    "for i, d in enumerate(TRAIN_DIRS):\n",
    "    nr_triples = list(range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1))[i]\n",
    "    \n",
    "    for filename in glob.iglob(d+'/**', recursive=False):\n",
    "        if os.path.isfile(filename): # Filter dirs\n",
    "            #print('File:', filename)\n",
    "            \n",
    "            tree = ET.parse(filename)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            entries = root[0]\n",
    "            train_stats[nr_triples-MIN_NUM_TRIPLES] += len(entries)\n",
    "            \n",
    "            for entry in entries:\n",
    "                #print('Original triple set: ', entry[originaltripleset_index])\n",
    "                #print('Modified triple set: ', entry[modifiedtripleset_index])\n",
    "                \n",
    "                modified_triple_set = entry[modifiedtripleset_index]\n",
    "                unified_triple_set = []\n",
    "                \n",
    "                for triple in modified_triple_set:\n",
    "                    # Make a list containing a conjunction of all individual triples\n",
    "                    triple_list = [x.strip() for x in triple.text.split('|')]\n",
    "                    unified_triple_set += triple_list\n",
    "                    \n",
    "                verbalizations = entry[first_lexical_index:]\n",
    "\n",
    "                for verbalization in verbalizations:\n",
    "                    if verbalization.text.strip() == '':\n",
    "                        continue\n",
    "                    #print('Text:', verbalization, verbalization.tag, verbalization.attrib, verbalization.text)\n",
    "                    #print('Trip:', triple, triple.tag, triple.attrib, triple.text)\n",
    "\n",
    "                    train[i].append({ 'category': entry.attrib['category'],\n",
    "                                      'id': entry.attrib['eid'],\n",
    "                                      'triple_cnt': nr_triples,\n",
    "                                      'text': verbalization.text,\n",
    "                                      'triple': unified_triple_set,\n",
    "                                    })\n",
    "                        \n",
    "print(train)\n",
    "print(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of test: test[target_nr_triples][element_id]['target_attribute']\n",
    "test = [[] for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1)]\n",
    "\n",
    "# Documents how many entries there are per number of triples\n",
    "test_stats = [0 for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1)]\n",
    "\n",
    "# Iterate through all files per number of triples and per category and load data\n",
    "for i, d in enumerate(TEST_DIRS):\n",
    "    nr_triples = list(range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1))[i]\n",
    "    \n",
    "    for filename in glob.iglob(d+'/**', recursive=False):\n",
    "        if os.path.isfile(filename): # Filter dirs\n",
    "            #print('File:', filename)\n",
    "            \n",
    "            tree = ET.parse(filename)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            entries = root[0]\n",
    "            test_stats[nr_triples-MIN_NUM_TRIPLES] += len(entries)\n",
    "            \n",
    "            for entry in entries:\n",
    "                #print('Original triple set: ', entry[originaltripleset_index])\n",
    "                #print('Modified triple set: ', entry[modifiedtripleset_index])\n",
    "                \n",
    "                modified_triple_set = entry[modifiedtripleset_index]\n",
    "                unified_triple_set = []\n",
    "                \n",
    "                for triple in modified_triple_set:\n",
    "                    # Make a list containing a conjunction of all individual triples\n",
    "                    triple_list = [x.strip() for x in triple.text.split('|')]\n",
    "                    unified_triple_set += triple_list\n",
    "                    \n",
    "                verbalizations = entry[first_lexical_index:]\n",
    "\n",
    "                for verbalization in verbalizations:\n",
    "                    if verbalization.text.strip() == '':\n",
    "                        continue\n",
    "                    #print('Text:', verbalization, verbalization.tag, verbalization.attrib, verbalization.text)\n",
    "                    #print('Trip:', triple, triple.tag, triple.attrib, triple.text)\n",
    "\n",
    "                    test[i].append({ 'category': entry.attrib['category'],\n",
    "                                      'id': entry.attrib['eid'],\n",
    "                                      'triple_cnt': nr_triples,\n",
    "                                      'text': verbalization.text,\n",
    "                                      'triple': unified_triple_set,\n",
    "                                    })\n",
    "                        \n",
    "print(test)\n",
    "print(test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spilt Train Data into Train and Dev (for intermindiate validation throughout training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of train data reserved for validation throughout training\n",
    "dev_percentage = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dev dataset\n",
    "dev = [[] for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1)]\n",
    "\n",
    "# Sample number of dev instances per number of triples\n",
    "dev_stats = [int(dev_percentage * train_stats[i]) for i in range(0, MAX_NUM_TRIPLES+1-MIN_NUM_TRIPLES)]\n",
    "\n",
    "print('Samples per nr of triples:', dev_stats)\n",
    "\n",
    "# Sample indices to be reserved for dev dataset for each nr of triples\n",
    "dev_indices = [random.sample(range(0, len(train[i])), dev_stats[i]) for i in range(0, MAX_NUM_TRIPLES+1-MIN_NUM_TRIPLES)]\n",
    "for i in range(len(dev_indices)):\n",
    "    dev_indices[i].sort(reverse=True)\n",
    "\n",
    "# Copy selected dev-entries into dev & delete all duplicates/related entries from train dataset\n",
    "for nr_triples in range(0, MAX_NUM_TRIPLES+1-MIN_NUM_TRIPLES):\n",
    "    \n",
    "    # Iterate through all indices reserved for validation set (per nr of triples)\n",
    "    for index in dev_indices[nr_triples]:\n",
    "        \n",
    "        # Select index'th train entry (to become dev/validation data)\n",
    "        selected_entry = train[nr_triples][index]\n",
    "        \n",
    "        # Extract indentifying attributes\n",
    "        entrys_category = selected_entry['category']\n",
    "        entrys_idx = selected_entry['id']\n",
    "        \n",
    "        # Put selected entry into dev set\n",
    "        dev[nr_triples].append(selected_entry)\n",
    "        \n",
    "        # Find all entries of matching index & category and remove them from train data\n",
    "        for entry in train[nr_triples]:\n",
    "            if entry['id'] == entrys_idx and entry['category'] == entrys_category:\n",
    "                train[nr_triples].remove(entry)\n",
    "                \n",
    "print(dev)\n",
    "print(dev_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimal number of triples:', MIN_NUM_TRIPLES)\n",
    "print('Maximal number of triples:', MAX_NUM_TRIPLES)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Training: ')\n",
    "for nr_triples in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1):\n",
    "    print('Given %i triples per sentence: ' % nr_triples)\n",
    "    print('Number of combinations of triples and verbalizations:', len(train[nr_triples-MIN_NUM_TRIPLES]))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dev: ')\n",
    "for nr_triples in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1):\n",
    "    print('Given %i triples per sentence: ' % nr_triples)\n",
    "    print('Number of combinations of triples and verbalizations:', len(dev[nr_triples-MIN_NUM_TRIPLES]))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Testing: ')\n",
    "for nr_triples in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1):\n",
    "    print('Given %i triples per sentence: ' % nr_triples)\n",
    "    print('Number of combinations of triples and verbalizations:', len(test[nr_triples-MIN_NUM_TRIPLES]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation (NMT) Model Definition \n",
    "\n",
    "## TODO: needs updating\n",
    "\n",
    "## Idea:\n",
    "1. Encoder: \n",
    "1.1 Input==Word Embedding; \n",
    "1.2 Output==Context Vector (that is: Encoding of sentence; contained in hidden state after having observed last embedding)\n",
    "\n",
    "2. Decoder:\n",
    "2.1 Input==Context Vector\n",
    "2.2 Output==Probability distribution over output vocab\n",
    "\n",
    "3. Seq2Seq model: Combining the two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Attention Model\n",
    "This model implements the Soft Attention model presented in http://proceedings.mlr.press/v37/xuc15.pdf. \n",
    "1. Attention energies (i.e. energy per annotation vector) get computed: $e_{ti}=f_{att}(a_i,h_{t−1})$. Note that this formula implies that the Decoder's previous hidden state $h_{t-1}$ needs to be appended to each individual annotation vector $a_i$ before feeding their concatenation through a fully-connected layer $f_{att}$. \n",
    "2. Attention weights $\\alpha$ get computed from the aforementioned energies: $\\alpha_t = softmax(e_t)$, where $\\alpha_{ti} = \\frac{exp(e_{ti})}{\\sum^L_{k=1} exp(e_{tk})}$.\n",
    "\n",
    "Note: $t$ stands for time, while $i$ identifies the particular annotation vector currently under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 annotation_size,  # Tuple: (num_annotations, num_features_per_annotation)\n",
    "                 hidden_len        # Number of nodes in Decoder's hidden state weight matrix\n",
    "                ):\n",
    "        \n",
    "        super(SoftAttention, self).__init__()\n",
    "        #print('SA INIT')\n",
    "        # Variables\n",
    "        self.num_annotations = annotation_size[0]\n",
    "        self.annotation_features = annotation_size[1]\n",
    "        self.hidden_size = hidden_len\n",
    "        \n",
    "        # Layers\n",
    "        self.attn = nn.Linear(self.annotation_features + self.hidden_size, 1, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        #print('an size:', annotation_size) # 8x96\n",
    "        #print('an features (96?):', self.annotation_features) # 96\n",
    "        #print('hid size:', hidden_len)     # 96\n",
    "        \n",
    "    def forward(self, annotations, prev_hidden):\n",
    "        \n",
    "        # Repeat prev_hidded X times to append it to each of the annotation vectors (per batch element)\n",
    "        repeated_hidden = torch.cat(\n",
    "            [\n",
    "                torch.repeat_interleave(hid, repeats=self.num_annotations, dim=0).unsqueeze(0)\n",
    "                for hid in prev_hidden.split(1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Append previous hidden state to all annotation vectors (for each individual batch element)\n",
    "        # Input to attention weight calculation\n",
    "        #print('SA:', annotations.size(), repeated_hidden.size())\n",
    "        input = torch.cat((annotations, repeated_hidden), dim=2)\n",
    "        #print('Input size:', input.size())\n",
    "        #print(self.attn)\n",
    "        \n",
    "        # Compute the relative attention scores per feaure (e_{ti}=f_{att}(a_i,h_{t−1}) from paper)\n",
    "        energies = self.attn(input)\n",
    "        \n",
    "        #print('energies...')\n",
    "        \n",
    "        # Compute final attention weights (i.e. alpha)\n",
    "        attn_weights = self.softmax(energies)\n",
    "        #print('attn_weights:', attn_weights.size())\n",
    "        return attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder itself (employing Soft Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 annotation_size,      # Size of annotation vectors produced by Encoder\n",
    "                 out_vocab_size,       # How many words there are in the RDF-output language\n",
    "                 embedding_dim,        # Length of a word embedding\n",
    "                 hidden_dim,           # Nr hidden nodes\n",
    "                 output_dim,           # Vocab size\n",
    "                 bidirectional=False,  # Whether to have bidirectional GRU\n",
    "                 n_layers=1,           # Nr layers in GRU\n",
    "                 drop_prob=0.2         # Percent of node-dropouts\n",
    "                ):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 1 if not bidirectional else 2  # TODO: make use of it...\n",
    "        \n",
    "        self.attn = SoftAttention(annotation_size=annotation_size, hidden_len=hidden_dim)\n",
    "        self.gru = nn.GRU(annotation_size[1]+embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, \n",
    "                annotations,  # Static annotation vectors (for each batch element)\n",
    "                embeddings,   # Word embeddings of most recently generated word (per batch element)\n",
    "                h_old         # Previous hidden state per batch element\n",
    "               ):\n",
    "        #print('Decoder forward:')\n",
    "        #print('embeddings:\\t', embeddings.size())\n",
    "        #print('h_old:\\t\\t', h_old.size())\n",
    "        \n",
    "        annotation_weights = self.attn(annotations, h_old.squeeze())#.unsqueeze(2)\n",
    "        #print('annotations:', annotations.size())\n",
    "        #print('annotation_weights:', annotation_weights.size())\n",
    "        weighted_annotations = annotations * annotation_weights\n",
    "        #print('weighted_annotations:', weighted_annotations.size())\n",
    "        context_vectors = torch.sum(weighted_annotations, dim=1)\n",
    "        #print('context_vectors:', context_vectors.size())\n",
    "        \n",
    "        x = torch.cat((context_vectors, embeddings), dim=1)\n",
    "        #print('x:', x.size())\n",
    "        x = x.unsqueeze(1) # Add une dimension for 'sequence'\n",
    "        \n",
    "        #print('Decoder x:', x.size(), 'h_old:', h_old.size())\n",
    "        #print(self.gru)\n",
    "        out, h = self.gru(x, h_old)\n",
    "        out = out.squeeze()\n",
    "        out = self.softmax(self.fc(self.relu(out)))\n",
    "        #print('h:', h.size())\n",
    "        return out, h\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, annotation_vectors):\n",
    "        # Mean of annotation vector per batch element\n",
    "        # Assumes that number of hidden nodes == number annotation features\n",
    "        hidden = torch.mean(annotation_vectors, dim=1)#.to(device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder for seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use it only if required otherwise can be removed.\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 embedding_dim,        # Length of a word embedding\n",
    "                 hidden_dim,           # Nr hidden nodes\n",
    "                 n_layers = 1,         # Nr layers in GRU\n",
    "                 dropout =0.2\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch size, emb dim] \n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs =\n",
    "        #hidden =\n",
    "        #cell =\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, \n",
    "            word_embeddings,        # Decoder's word embeddings\n",
    "            word2idx,               # \n",
    "            idx2word,               # \n",
    "            encoder,                # \n",
    "            decoder,                # \n",
    "            tokenizer,              # \n",
    "            loss_fn,                # \n",
    "            max_len=7,              # \n",
    "            batch_size=32,          # \n",
    "            compute_grads=False,    # \n",
    "            targets=None,           # \n",
    "            return_textual=False    # Whether to return predictions in index-form (default) or as textual strings\n",
    "           ):\n",
    "    \n",
    "    print('In predict:')\n",
    "    \n",
    "    accumulated_loss = 0.\n",
    "    \n",
    "    # Init documentation of predictions\n",
    "    predicted_indices = torch.zeros([batch_size, max_len]).to(device) # Numeric\n",
    "    if return_textual:\n",
    "        predicted_words = ['']*batch_size\n",
    "    \n",
    "    # Tokenize sampled minibatch sentences\n",
    "    inputs = tokenizer(x, \n",
    "                       return_tensors=\"pt\",     # Return tensors in pt=PyTorch format\n",
    "                       padding=True,            # Pad all sentences in mini-batch to have the same length\n",
    "                       add_special_tokens=True).to(device) # Add \"Start of sequence\", \"End of sequence\", ... tokens. \n",
    "    \n",
    "    #print('Tokenized Inputs:', inputs)\n",
    "    \n",
    "    # Encode sentences: Pass tokenization output-dict-contents to model\n",
    "    outputs = encoder(**inputs)\n",
    "    #print('Got outputs:', outputs)\n",
    "\n",
    "    # Retrieve hidden state to be passed into Decoder as annotation vectors\n",
    "    # Reshape to get a set of 8 feature vectors from last hidden state\n",
    "    annotations = outputs.last_hidden_state[:, -1, :].reshape(batch_size,8,-1).to(device)\n",
    "    #print('Annotations size after cropping & reshape:', annotations.size())\n",
    "\n",
    "    # Init Decoder's hidden state\n",
    "    hidden = decoder.init_hidden(annotations).unsqueeze(0).to(device)\n",
    "    #print('Initial hidden size:', hidden.size(), 'given annotations:', annotations.size())\n",
    "    \n",
    "    # Construct initial embeddings (start tokens)\n",
    "    embeddings = word_embeddings(torch.zeros([batch_size], dtype=int).to(device)).to(device)\n",
    "    \n",
    "    for t in range(max_len):\n",
    "        #print('START OF ITERATION', t)\n",
    "        # Get decodings (aka prob distrib. over output vocab per batch element) for time step t\n",
    "        prob_dist, hidden = decoder(annotations, # Static vector containing annotations per batch element \n",
    "                                    embeddings,  # Word embedding predicted last iteration (per batch element)\n",
    "                                    hidden       # Decoder's hidden state of last iteratipn per batch element\n",
    "                                    )\n",
    "\n",
    "        # Get predicted word index from predicted probability distribution (per batch element)\n",
    "        word_index = torch.max(prob_dist, dim=1).indices\n",
    "        #print('Predicted word indices batch:', word_index)\n",
    "        \n",
    "        # Get corresponding word embedding (by index; per batch element)\n",
    "        embedding = word_embeddings(word_index.to(device))\n",
    "        \n",
    "        # TODO: optional teacher forcing?\n",
    "\n",
    "        # Record predicted words\n",
    "        predicted_indices[:, t] = word_index\n",
    "        #print('Predicted indices:', predicted_indices)\n",
    "        \n",
    "        # Record textual words if required\n",
    "        if return_textual:\n",
    "            \n",
    "            # Get predicted word (per batch element)\n",
    "            predicted_word = [idx2word(batch_element) for batch_element in word_index]\n",
    "        \n",
    "            for e in range(batch_size):\n",
    "                predicted_words[e] += predicted_word[e]\n",
    "\n",
    "        if compute_grads:\n",
    "            \n",
    "            #print('prob_dist:', prob_dist.size())\n",
    "            #print('targets:', targets[:, t].size(), targets[:, t])\n",
    "            \n",
    "            # Compute (averaged over all batch elements given current time step t)\n",
    "            loss = loss_fn(prob_dist, targets[:, t]).to(device)\n",
    "\n",
    "            # Compute & back-propagate gradients\n",
    "            loss.backward(retain_graph=True)\n",
    "            \n",
    "            # Document loss\n",
    "            accumulated_loss += loss.item()\n",
    "        #print('END OF ITERATION', t)\n",
    "            \n",
    "    ret_object = {\n",
    "        'predicted_indices': predicted_indices,\n",
    "    }\n",
    "    \n",
    "    print('Targets:\\n', targets)\n",
    "    print('Predicted idxs:\\n', predicted_indices)\n",
    "    \n",
    "    if compute_grads:\n",
    "        ret_object['loss'] = accumulated_loss\n",
    "        #print('Accumulated loss:', accumulated_loss)\n",
    "    if return_textual:\n",
    "        ret_object['predicted_words'] = predicted_words\n",
    "        #print(\"Predicted words:\", predicted_words)\n",
    "    #print(\"Returning from predict\")\n",
    "    return ret_object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rdf_vocab_constructor(raw_vocab):\n",
    "    #print(raw_vocab)\n",
    "    vocab_count, word2idx, idx2word = 3, {'START': 0, 'PAD': 1, 'END': 2}, {0: 'START', 1: 'PAD', 2: 'END'}\n",
    "    \n",
    "    for partition in raw_vocab: # Different partitions with respect to nr or triples per sentence\n",
    "        for train_instance in partition:\n",
    "            triple = train_instance['triple']\n",
    "            for token in triple:\n",
    "                if token not in word2idx:\n",
    "                    word2idx[token] = vocab_count\n",
    "                    idx2word[vocab_count] = token\n",
    "                    vocab_count += 1\n",
    "    return vocab_count, word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_data, \n",
    "          val_data,  \n",
    "          epochs, \n",
    "          minibatch_size=32,\n",
    "          embedding_dim=300,\n",
    "          eval_frequency=10, # Every how many epochs to run intermediate evaluation\n",
    "          learning_rate=0.0001\n",
    "         ):\n",
    "    \n",
    "    # Construct RDF vocab\n",
    "    vocab_count, word2idx, idx2word = rdf_vocab_constructor(train_data)\n",
    "    \n",
    "    # Construct embeddings\n",
    "    rdf_vocab = nn.Embedding(num_embeddings=vocab_count, embedding_dim=embedding_dim, padding_idx=0).to(device)\n",
    "    \n",
    "    # Define model\n",
    "    encoder = bert_model.to(device)\n",
    "    decoder = Decoder(\n",
    "        annotation_size=(8,96),    # Size of annotation vectors produced by Encoder\n",
    "        out_vocab_size=vocab_count, # How many words there are in the RDF-output language\n",
    "        embedding_dim=300,          # Length of a word embedding\n",
    "        hidden_dim=96,             # Nr hidden nodes\n",
    "        output_dim=vocab_count,             # Vocab size\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # For both train and validation data & for all number of tuples per sentence \n",
    "    # (in [MIN_NUM_TRIPLES, MAX_NUM_TRIPLES]), get the nr of train-/test instances\n",
    "    len_x_train = [len(train_set) for train_set in train]\n",
    "    len_x_val = [len(val_set) for val_set in dev]\n",
    "    \n",
    "    # Development of both train- and validation losses over course of training\n",
    "    train_losses, val_losses = [0.]*epochs, [0.]*epochs\n",
    "    \n",
    "    print('Starting training.')\n",
    "    \n",
    "    # Train\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch:', epoch)\n",
    "        \n",
    "        train_loss, eval_loss = 0., 0.\n",
    "        \n",
    "        # Reset gradients\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        \n",
    "        # Perform own train step for each nr of triples per sentence separately\n",
    "        for i, nt in enumerate(range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1)):\n",
    "            print(str(i) + '. Condition:', nt, 'triples per sentence.')\n",
    "            \n",
    "            # Sample minibatch indices\n",
    "            minibatch_idx = random.sample(population=range(len_x_train[i]), k=minibatch_size)\n",
    "            #print('MB indices:', minibatch_idx)\n",
    "            \n",
    "            # Number of tokens to be predicted (per batch element)\n",
    "            num_preds = nt*3+1 # = nr triples * 3 + stop_token \n",
    "            #print('Number of predictions:', num_preds)\n",
    "            \n",
    "            # Construct proper minibatch\n",
    "            inputs = [train_data[i][idx]['text'] for idx in minibatch_idx]\n",
    "            targets = torch.ones([minibatch_size, num_preds], dtype=int).to(device)\n",
    "            \n",
    "            #print('Inputs:', inputs)\n",
    "            #print('Targets:', targets)\n",
    "            \n",
    "            for mb_i, idx in enumerate(minibatch_idx):\n",
    "                #print('Text:', train_data[i][idx]['text'])\n",
    "                #print('Triple:', train_data[i][idx]['triple'])\n",
    "                for t, token in enumerate(train_data[i][idx]['triple']):\n",
    "                    targets[mb_i, t] = word2idx[token]\n",
    "            targets[:, -1] = 2  # 2 = Stop word index\n",
    "            \n",
    "            #print('Processed targets:', targets)\n",
    "            #print('Predicting:')\n",
    "            \n",
    "            # Predict\n",
    "            ret_object = predict(inputs,\n",
    "                                 rdf_vocab,              # Decoder's word embeddings\n",
    "                                 word2idx,               # \n",
    "                                 idx2word,               # \n",
    "                                 encoder,                # \n",
    "                                 decoder,                # \n",
    "                                 tokenizer,              # \n",
    "                                 loss,                   # \n",
    "                                 max_len=num_preds,      # Nr of tokens to be predicted\n",
    "                                 batch_size=32,          # \n",
    "                                 compute_grads=True,     # \n",
    "                                 targets=targets,        # \n",
    "                                 return_textual=False    # Whether to return predictions in index-form (default) or as textual strings\n",
    "                                )\n",
    "            \n",
    "            print('Return object:', ret_object)\n",
    "            train_loss += ret_object['loss']\n",
    "            #print(\"Returned loss:\", ret_object['loss'])\n",
    "            \n",
    "        # Apply gradients\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        #print('Optimizations performed.')\n",
    "        \n",
    "        # Intermediate evaluation\n",
    "        \n",
    "        # Save losses\n",
    "        train_losses[epoch] = train_loss\n",
    "        \n",
    "    return train_losses, val_losses, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, encoder, decoder = training(train, dev, epochs=100, learning_rate=0.0001)\n",
    "print('Train losses:', train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "### Used exclusively for evaluation on test data after training is fuly finished"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation - Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the BLEU score for multiple sentence.\n",
    "def calculate_bleu(data, train, dev, model, max_len = 7):\n",
    "    \n",
    "    trgs = []\n",
    "    pred_trgs = []       \n",
    "    src = dev\n",
    "    trg = test\n",
    "    # Get the data and feed it into pred_trg after Seq2seq\n",
    "    #pred_trg = pred_trg[:-1]      \n",
    "    #pred_trgs.append(pred_trg)\n",
    "    #trgs.append([trg])\n",
    "        \n",
    "    return corpus_bleu(pred_trgs, trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
