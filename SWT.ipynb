{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text2rdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Bert\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Data-sub-set (given restricted nr of triples per sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many triples to train and test system on (min: 1, max: 7)\n",
    "MIN_NUM_TRIPLES = 1\n",
    "MAX_NUM_TRIPLES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths where to retrieve data from\n",
    "DS_BASE_PATH = './WebNLG/'\n",
    "\n",
    "TRAIN_PATH = DS_BASE_PATH + 'train/'\n",
    "TEST_PATH = DS_BASE_PATH + 'dev/'\n",
    "\n",
    "TRAIN_DIRS = [ TRAIN_PATH + str(i) + 'triples/' for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1) ]\n",
    "TEST_DIRS  = [ TEST_PATH  + str(i) + 'triples/' for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print selected directories\n",
    "print('Train dirs:', TRAIN_DIRS)\n",
    "print('Test  dirs:', TEST_DIRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Settings (do not touch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originaltripleset_index = 0  # Index of 'originaltripleset' attribute in XML entry\n",
    "modifiedtripleset_index = 1  # Index of 'modifiedtripleset' attribute in XML entry\n",
    "first_lexical_index = 2      # Index as of which verbalizations of RDF triples start in entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of train: train[target_nr_triples][element_id]['target_attribute']\n",
    "train = [[] for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1)]\n",
    "\n",
    "# Documents how many entries there are per number of triples\n",
    "train_stats = [0 for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1)]\n",
    "\n",
    "# Iterate through all files per number of triples and per category and load data\n",
    "for i, d in enumerate(TRAIN_DIRS):\n",
    "    nr_triples = list(range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1))[i]\n",
    "    \n",
    "    for filename in glob.iglob(d+'/**', recursive=False):\n",
    "        if os.path.isfile(filename): # Filter dirs\n",
    "            #print('File:', filename)\n",
    "            \n",
    "            tree = ET.parse(filename)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            entries = root[0]\n",
    "            train_stats[nr_triples-MIN_NUM_TRIPLES] += len(entries)\n",
    "            \n",
    "            for entry in entries:\n",
    "                #print('Original triple set: ', entry[originaltripleset_index])\n",
    "                #print('Modified triple set: ', entry[modifiedtripleset_index])\n",
    "                \n",
    "                modified_triple_set = entry[modifiedtripleset_index]\n",
    "                \n",
    "                for triple in modified_triple_set:\n",
    "                    \n",
    "                    verbalizations = entry[first_lexical_index:]\n",
    "                \n",
    "                    for verbalization in verbalizations:\n",
    "                        #print('Text:', verbalization, verbalization.tag, verbalization.attrib, verbalization.text)\n",
    "                        #print('Trip:', triple, triple.tag, triple.attrib, triple.text)\n",
    "                        \n",
    "                        train[i].append({ 'category': entry.attrib['category'],\n",
    "                                          'id': entry.attrib['eid'],\n",
    "                                          'triple_cnt': nr_triples,\n",
    "                                          'text': verbalization.text,\n",
    "                                          'triple': [text.strip() for text in triple.text.split('|')]\n",
    "                                        })\n",
    "                        \n",
    "print(train)\n",
    "print(train_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of test: test[target_nr_triples][element_id]['target_attribute']\n",
    "test = [[] for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1)]\n",
    "\n",
    "# Documents how many entries there are per number of triples\n",
    "test_stats = [0 for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1)]\n",
    "\n",
    "# Iterate through all files per number of triples and per category and load data\n",
    "for i, d in enumerate(TEST_DIRS):\n",
    "    nr_triples = list(range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1))[i]\n",
    "    \n",
    "    for filename in glob.iglob(d+'/**', recursive=False):\n",
    "        if os.path.isfile(filename): # Filter dirs\n",
    "            #print('File:', filename)\n",
    "            \n",
    "            tree = ET.parse(filename)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            entries = root[0]\n",
    "            test_stats[nr_triples-MIN_NUM_TRIPLES] += len(entries)\n",
    "            \n",
    "            for entry in entries:\n",
    "                #print('Original triple set: ', entry[originaltripleset_index])\n",
    "                #print('Modified triple set: ', entry[modifiedtripleset_index])\n",
    "                \n",
    "                modified_triple_set = entry[modifiedtripleset_index]\n",
    "                \n",
    "                for triple in modified_triple_set:\n",
    "                    \n",
    "                    verbalizations = entry[first_lexical_index:]\n",
    "                \n",
    "                    for verbalization in verbalizations:\n",
    "                        #print('Text:', verbalization, verbalization.tag, verbalization.attrib, verbalization.text)\n",
    "                        #print('Trip:', triple, triple.tag, triple.attrib, triple.text)\n",
    "                        \n",
    "                        test[i].append({ 'category': entry.attrib['category'],\n",
    "                                          'id': entry.attrib['eid'],\n",
    "                                          'triple_cnt': nr_triples,\n",
    "                                          'text': verbalization.text,\n",
    "                                          'triple': [text.strip() for text in triple.text.split('|')]\n",
    "                                        })\n",
    "                        \n",
    "print(test)\n",
    "print(test_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spilt Train Data into Train and Dev (for intermindiate validation throughout training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of train data reserved for validation throughout training\n",
    "dev_percentage = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init dev dataset\n",
    "dev = [[] for i in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1)]\n",
    "\n",
    "# Sample number of dev instances per number of triples\n",
    "dev_stats = [int(dev_percentage * train_stats[i]) for i in range(0, MAX_NUM_TRIPLES+1-MIN_NUM_TRIPLES)]\n",
    "\n",
    "print('Samples per nr of triples:', dev_stats)\n",
    "\n",
    "# Sample indices to be reserved for dev dataset for each nr of triples\n",
    "dev_indices = [random.sample(range(0, len(train[i])), dev_stats[i]) for i in range(0, MAX_NUM_TRIPLES+1-MIN_NUM_TRIPLES)]\n",
    "for i in range(len(dev_indices)):\n",
    "    dev_indices[i].sort(reverse=True)\n",
    "\n",
    "# Copy selected dev-entries into dev & delete all duplicates/related entries from train dataset\n",
    "for nr_triples in range(0, MAX_NUM_TRIPLES+1-MIN_NUM_TRIPLES):\n",
    "    \n",
    "    # Iterate through all indices reserved for validation set (per nr of triples)\n",
    "    for index in dev_indices[nr_triples]:\n",
    "        \n",
    "        # Select index'th train entry (to become dev/validation data)\n",
    "        selected_entry = train[nr_triples][index]\n",
    "        \n",
    "        # Extract indentifying attributes\n",
    "        entrys_category = selected_entry['category']\n",
    "        entrys_idx = selected_entry['id']\n",
    "        \n",
    "        # Put selected entry into dev set\n",
    "        dev[nr_triples].append(selected_entry)\n",
    "        \n",
    "        # Find all entries of matching index & category and remove them from train data\n",
    "        for entry in train[nr_triples]:\n",
    "            if entry['id'] == entrys_idx and entry['category'] == entrys_category:\n",
    "                train[nr_triples].remove(entry)\n",
    "                \n",
    "print(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minimal number of triples:', MIN_NUM_TRIPLES)\n",
    "print('Maximal number of triples:', MAX_NUM_TRIPLES)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Training: ')\n",
    "for nr_triples in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1):\n",
    "    print('Given %i triples per sentence: ' % nr_triples)\n",
    "    print('Number of combinations of triples and verbalizations:', len(train[nr_triples-MIN_NUM_TRIPLES]))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dev: ')\n",
    "for nr_triples in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1):\n",
    "    print('Given %i triples per sentence: ' % nr_triples)\n",
    "    print('Number of combinations of triples and verbalizations:', len(dev[nr_triples-MIN_NUM_TRIPLES]))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Testing: ')\n",
    "for nr_triples in range(MIN_NUM_TRIPLES, MAX_NUM_TRIPLES+1):\n",
    "    print('Given %i triples per sentence: ' % nr_triples)\n",
    "    print('Number of combinations of triples and verbalizations:', len(test[nr_triples-MIN_NUM_TRIPLES]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation (NMT) Model Definition \n",
    "\n",
    "## TODO: needs updating\n",
    "\n",
    "## Idea:\n",
    "1. Encoder: \n",
    "1.1 Input==Word Embedding (or Letter embedding); \n",
    "1.2 Output==Context Vector (that is: Encoding of sentence; contained in hidden state after having observed last embedding)\n",
    "\n",
    "2. Decoder:\n",
    "2.1 Input==Context Vector\n",
    "2.2 Output==Probability distribution over output vocab (might be words or letters alternatively)\n",
    "\n",
    "3. Seq2Seq model: Combining the two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)#.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Attention Model\n",
    "This model implements the Soft Attention model presented in http://proceedings.mlr.press/v37/xuc15.pdf. \n",
    "1. Attention energies (i.e. energy per annotation vector) get computed: $e_{ti}=f_{att}(a_i,h_{t−1})$. Note that this formula implies that the Decoder's previous hidden state $h_{t-1}$ needs to be appended to each individual annotation vector $a_i$ before feeding their concatenation through a fully-connected layer $f_{att}$. \n",
    "2. Attention weights $\\alpha$ get computed from the aforementioned energies: $\\alpha_t = softmax(e_t)$, where $\\alpha_{ti} = \\frac{exp(e_{ti})}{\\sum^L_{k=1} exp(e_{tk})}$.\n",
    "\n",
    "Note: $t$ stands for time, while $i$ identifies the particular annotation vector currently under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 annotation_size,  # Tuple: (num_annotations, num_features_per_annotation)\n",
    "                 hidden_len        # Number of nodes in Decoder's hidden state weight matrix\n",
    "                ):\n",
    "        \n",
    "        super(SoftAttention, self).__init__()\n",
    "        \n",
    "        # Variables\n",
    "        self.num_annotations = annotation_size[0]\n",
    "        self.annotation_features = annotation_size[1]\n",
    "        self.hidden_size = hidden_len\n",
    "        \n",
    "        # Layers\n",
    "        self.attn = nn.Linear(self.annotation_features + self.hidden_size, 1, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, annotations, prev_hidden):\n",
    "        \n",
    "        # Repeat prev_hidded X times to append it to each of the annotation vectors (per batch element)\n",
    "        repeated_hidden = torch.cat(\n",
    "            [\n",
    "                torch.repeat_interleave(hid, repeats=self.num_annotations, dim=0).unsqueeze(0)\n",
    "                for hid in prev_hidden.split(1)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Append previous hidden state to all annotation vectors (for each individual batch element)\n",
    "        # Input to attention weight calculation\n",
    "        input = torch.cat((annotations, repeated_hidden), dim=2)\n",
    "        \n",
    "        # Compute the relative attention scores per feaure (e_{ti}=f_{att}(a_i,h_{t−1}) from paper)\n",
    "        energies = self.attn(input)\n",
    "        \n",
    "        # Compute final attention weights (i.e. alpha)\n",
    "        attn_weights = self.softmax(energies)\n",
    "        \n",
    "        return attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder itself (employing Soft Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 annotation_size, # Size of annotation vectors produced by Encoder\n",
    "                 hidden_nodes,    # Size of this module's (i.e. Decoder's) hidden state\n",
    "                 out_vocab_size,  # How many words there are in the RDF-output language\n",
    "                 dropout_p=0.1    # Percent of node-dropouts\n",
    "                ):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Variables\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        \n",
    "        # Layers\n",
    "        self.attn = SoftAttention(annotation_size=annotation_size, hidden_len=hidden_nodes)\n",
    "        self.rnn = nn.GRU() # TODO\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        \n",
    "    def forward(self, annotations, prev_hidden):\n",
    "        \n",
    "        # TODO (don't forget about Dropout)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def init_hidden(self, annotation_vecs):\n",
    "        return None # TODO: Mean over annotation_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes input and feeds it through both Encoder and (subsequently) Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: These are just some examples of fuctionalities to be used inside train loop\n",
    "# Documentation: https://huggingface.co/transformers/model_doc/bert.html\n",
    "\n",
    "# Sample mini-batch\n",
    "mini_batch = [\"Hello, my cat is cute\", \n",
    "              \"It is not\", \n",
    "              \"HERE I Am, Bye\"]\n",
    "\n",
    "# Tokenize sampled mini-batch sentences\n",
    "inputs = tokenizer(mini_batch, \n",
    "                   return_tensors=\"pt\",     # Return tensors in pt=PyTorch format\n",
    "                   padding=True,            # Pad all sentences in mini-batch to have the same length\n",
    "                   add_special_tokens=True) # Add \"Start of sequence\", \"End of sequence\", ... tokens. \n",
    "\n",
    "# Encode sentences: Pass tokenization output-dict-contents to model\n",
    "outputs = model(**inputs) \n",
    "\n",
    "# Retrieve hidden state to be passed into Decoder \n",
    "last_hidden_states = outputs.last_hidden_state  \n",
    "\n",
    "# Print...\n",
    "print(inputs)\n",
    "print(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swt",
   "language": "python",
   "name": "swt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
